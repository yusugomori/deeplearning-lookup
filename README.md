# Machine Learning / Deep Learning Lookup

## A

- Accuracy
- Activation Function
- AdaBoost (Adaptive Boosting)
- Adadelta
- Adagrad
- Adam
- Adaptive Boosting
- AlexNet
- AllenNLP
- Attention
- Area Under Curve
- AUC (Area Under Curve)
- Autoencoder
- Average Pooling

## B

- Batch
- Batch Normalization
- BERT (Bidirectional Encoder Representation from Transformer)
- Bidirectional Encoder Representation from Transformer
- Bidirectional RNN
- Boltzmann Machines

## C

- Caffe
- Chainer
- Cifar10
- Clustering
- CNN (Convolutional Neural Network)
- Common Objects in Context
- Conditional GAN
- Conditional Random Fields
- Confusion Matrix
- Convolution
- Convolutional Neural Network
- COCO (Common Objects in Context)
- CRF (Conditional Random Fields)
- Cross-Entropy Error

## D

- Data Augmentation
- DBN (Deep Belief Network)
- DCGAN (Deep Convolutional GAN)
- Decoder
- Deep Belief Network
- Deep Convolutional GAN
- Deep Learning
- Deep Q-Network
- Deep Reinforcement Learning
- Deep Residual Network
- Denoising Autoencoder
- Dense Layer
- DenseNet
- Depthwise Convolution
- Discriminator
- Doc2Vec
- Docker
- Double Q-Learning
- DQN (Deep Q-Network)
- DropConnect
- Dropout
- Dueling Network

## E

- Early Stopping
- ELMo
- ELU (Exponential Linear Unit)
- Embedding
- Encoder
- Encoder-Decoder
- Ensemble Learning
- Epoch
- Error Function
- Experience Reply
- Exponential Linear Unit

## F

- Fashion MNIST
- Fast R-CNN
- Faster R-CNN
- F1 Score
- F-measure
- Forget Gate
- F値（Fち）

## G

- GAN (Generative Adversarial Network)
- Gated Recurrent Unit
- Gaussian Error Linear Unit
- GELU (Gaussian Error Linear Unit)
- Generative Adversarial Network
- Generative Query Network
- Generator
- GoogLeNet
- GQN (Generative Query Network)
- GradCAM
- Gradient Boosting
- Gradient Descent Method
- Group Convolution
- Group Normalization
- GRU (Gated Recurrent Unit)
- Gym

## H

- He Initialization
- Hidden Layer
- Hold-out Method
- Hyperbolic Tangent Function

## I

- ImageNet
- Inception
- Input Gate
- Input Layer
- Input Weight Conflict

## J

## K

- Keras
- Kernel Method
- k-fold
- KL Divergence (Kullback–Leibler Divergence)
- k-Fold Cross Validation
- k-Means
- k-Nearest Neighbor
- kNN (k-Nearest Neighbor)
- Kullback–Leibler Divergence
- k近傍法（kきんぼうほう）
- k分割交差検証法（kぶんかつこうさけんしょうほう）
- k平均法（kへいきんほう）

## L

- Label Smoothing
- Layer Normalization
- Leaky ReLU
- LeNet
- Logistic Regression
- Long Short-Term Memory
- LSTM (Long Short-Term Memory)

## M

- Max Pooling
- Mean Squared Error
- Mini-Batch
- MLP (Multi-Layer Perceptron)
- MNIST
- MobileNet
- Momentum
- MSE (Mean Squared Error)
- Multi-Layer Perceptron

## N

- NAC (Neural Accumulator)
- NALU (Neural Arithmetic Logic Unit)
- Nesterov
- Neural Accumulator
- Neural Arithmetic Logic Unit
- Neural Machine Translation
- NMF (Non-negative Matrix Factorization)
- NMT (Neural Machine Translation)
- Non-negative Matrix Factorization
- Normalization

## O

- One-hot encoding
- One-hot Vector
- Output Gate
- Output Layer
- Output Weight Conflict
- Overfitting
- Oversampling

## P

- Parametric ReLU
- PCA (Principal Component Analysis)
- Peephole Connection
- Pointwise Convolution
- Pooling
- Precision
- PReLU (Parametric ReLU)
- Principal Component Analysis
- Prioritized Experience Reply
- PyTorch

## Q

- Q-Learning
- Q学習（Qがくしゅう）

## R

- Random Forest
- Rainbow
- Randomized ReLU
- RBM (Restricted Boltzmann Machines)
- R-CNN
- Recall
- Receiver Operating Characteristic Curve
- Recurrent Neural Network
- Restricted Boltzmann Machines
- Reinforcement Learning
- ReLU (Rectified Linear Unit)
- ResNet
- RMSE (Root Mean Squared Error)
- RMSprop
- RNN (Recurrent Neural Network)
- ROC Curve (Receiver Operating Characteristic Curve)
- ROC曲線（ROCきょくせん）
- Root Mean Squared Error
- RReLU (Randomized ReLU)

## S

- Scaled Exponential Linear Units
- SDA (Stacked Denoising Autoencoder)
- Self-Attention
- SeLU (Scaled Exponential Linear Units)
- Semi-Supervised Learning
- Seq2Seq
- Sequence-to-Sequence
- SGD (Stochastic Gradient Descent)
- Sigmoid Function
- Singular Value Decomposition
- Softmax Function
- Stacked Denoising Autoencoder
- Steepest Descent
- Step Function
- Stochastic Gradient Descent
- Supervised Learning
- Support Vector Machine
- SVD (Singular Value Decomposition)
- SVM (Support Vector Machine)
- Swish

## T

- Tanh
- TD学習
- Teacher Forcing
- Temporal Difference Learning
- Tensor
- TensorFlow
- Toy Problem
- Transformer

## U

- U-Net
- Underfitting
- Undersampling
- Unsupervised Learning

## V

- VAE (Variational Autoencoder)
- Variational Autoencoder
- Variational Bayes
- VGG

## W

- WaveNet
- Wasserstein GAN
- WGAN (Wasserstein GAN)
- Word Embedding
- Word2Vec

## X

- Xavier Initialization
- Xception

## Y

- YOLO (You Only Look Once)
- You Only Look Once

## Z

- ZCA Whitening
- ZCA白色化（ZCAはくしょくか）
- ZF Net

## あ

- アンサンブル学習（あんさんぶるがくしゅう）
- アンダーサンプリング（あんだーさんぷりんぐ）
- アンダーフィッティング（あんだーふぃってぃんぐ）

## い

## う

## え

- エポック（えぽっく）

## お

- オーバーサンプリング（おーばーさんぷりんぐ）
- オーバーフィッティング（おーばーふぃってぃんぐ）

## か

- 確率的勾配降下法（かくりつてきこうばいこうかほう）
- 過学習（かがくしゅう）
- 隠れ層（かくれそう）
- 過剰適合（かじょうてきごう）
- 活性化関数（かっせいかかんすう）
- カーネル法（かーねるほう）

## き

- 強化学習（きょうかがくしゅう）
- 教師あり学習（きょうしありがくしゅう）
- 教師なし学習（きょうしなしがくしゅう）

## く

- クラスタリング（くらすたりんぐ）

## け

## こ

- 勾配降下法（こうばいこうかほう）
- 勾配ブースティング（こうばいぶーすてぃんぐ）
- 混同行列（こんどうぎょうれつ）

## さ

- 最急降下法（さいきゅうこうかほう）
- 再帰的ニューラルネットワーク（さいきてきにゅーらるねっとわーく）
- 再現率（さいげんりつ）
- 最大値プーリング（さいだいちぷーりんぐ）
- サポートベクターマシン

## し

- シグモイド関数（しぐもいどかんすう）
- 主成分分析（しゅせいぶんぶんせき）
- 出力重み衝突（しゅつりょくおもみしょうとつ）
- 出力ゲート（しゅつりょくげーと）
- 出力層（しゅつりょくそう）
- 条件付き確率場（じょうけんつきかくりつば）
- 深層学習（しんそうがくしゅう）
- 深層強化学習（しんそうきょうかがくしゅう）

## す

- ステップ関数（すてっぷかんすう）

## せ

- 正解率（せいかいりつ）
- 正規化（せいきか）
- 制限付きボルツマンマシン（せいげんつきぼるつまんましん）
- 全結合層（ぜんけつごうそう）

## そ

- ソフトマックス関数（そふとまっくすかんすう）

## た

- 多層パーセプトロン（たそうぱーせぷとろん）
- 畳み込み（たたみこみ）
- 畳み込みニューラルネットワーク（たたみこみにゅーらるねっとわーく）

## ち

- 注意（ちゅうい）/ 注意機構（ちゅういきこう）

## つ

## て

- ディープラーニング（でぃーぷらーにんぐ）
- 適合率（てきごうりつ）
- 敵対的生成ネットワーク（てきたいてきせいせいねっとわーく）
- データ拡張（でーたかくちょう）
- テンソル（てんそる）

## と

- トイプロブレム（といぷろぶれむ）
- 特異値分解（とくいちぶんかい）

## な

## に

- 入力重み衝突（にゅうりょくおもみしょうとつ）
- 入力ゲート（にゅうりょくげーと）
- 入力層（にゅうりょくそう）

## ぬ

## ね

## の

- 覗き穴結合（のぞきあなけつごう）

## は

- 白色化（はくしょくか）
- バッチ（ばっち）
- バッチ正規化（ばっちせいきか）
- 半教師あり学習（はんきょうしありがくしゅう）

## ひ

- 非負値行列因子分解（ひふちぎょうれついんしぶんかい）

## ふ

- プーリング（ぷーりんぐ）

## へ

- 平均値プーリング（へいきんちぷーりんぐ）
- 変分ベイズ（へんぶんべいず）

## ほ

- 忘却ゲート（ぼうきゃくげーと）
- ボルツマンマシン（ぼるつまんましん）
- ホールドアウト法（ほーるどあうとほう）

## ま

## み

- ミニバッチ（みにばっち）

## む

## め

## も

- モメンタム（もめんたむ）

## や

## ゆ

## よ

## ら

- ランダムフォレスト（らんだむふぉれすと）

## り

- リカレントニューラルネットワーク（りかれんとにゅーらるねっとわーく）

## る

## れ

## ろ

- ロジスティック回帰（ろじすてぃっくかいき）

## わ

## を

## ん
